{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vefVMj7z7Phg"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"uuS0YKdtjUJV"},"source":["**Multi Layer Perceptron (MLP) from scratch using Backpropogation**"]},{"cell_type":"markdown","metadata":{"id":"vq9Owa4ZjbEU"},"source":["**Layer**\n","Initialized as layer = Layer(input_dim, output_dim).  \n","We define *weights* and *biases* as the learnable parameters that gets updated by the optimization method.  \n","The forward pass computes $Z = weight^T.X + bias$\n","The update_weights function would update the *weights* and *biases* with their gradient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLfm74g170PR"},"outputs":[],"source":["class Layer:\n","    def __init__(self, input_dim, output_dim):\n","        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2 / input_dim)\n","        self.bias = np.zeros(output_dim)\n","        self.input = None\n","        self.output = None\n","    \n","    def forward(self, X):\n","        self.input = X\n","        self.output = np.dot(X, self.weights) + self.bias\n","        return self.output\n","    \n","    def update_weights(self, dW, db, learning_rate):\n","        self.weights -= learning_rate * dW\n","        self.bias -= learning_rate * db"]},{"cell_type":"markdown","metadata":{"id":"AkID4-UCkZ-5"},"source":["**Activation Layer**  \n","Supports both *Sigmoid* and *ReLU* activation.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LfWp9st8JVA"},"outputs":[],"source":["class Activation(object):\n","    def __init__(self, activation_type):\n","        # Initialize the activation function based on the activation type\n","        if activation_type == \"sigmoid\":\n","            self.activation_func = self.sigmoid\n","            self.activation_prime = self.sigmoid_prime\n","        elif activation_type == \"relu\":\n","            self.activation_func = self.relu\n","            self.activation_prime = self.relu_prime\n","\n","    def sigmoid(self, x):\n","        # Compute the sigmoid activation function\n","        return 1 / (1 + np.exp(-x))\n","\n","    def sigmoid_prime(self, x):\n","        # Compute the derivative of the sigmoid activation function\n","        sigmoid = self.sigmoid(x)\n","        return sigmoid * (1 - sigmoid)\n","\n","    def relu(self, x):\n","        # Compute the rectified linear unit (ReLU) activation function\n","        return np.maximum(0, x)\n","\n","    def relu_prime(self, x):\n","        # Compute the derivative of the rectified linear unit (ReLU) activation function\n","        return (x \u003e 0).astype(int)\n","\n","    def forward(self, input):\n","        # Compute the forward pass through the activation function\n","        self.input = input\n","        self.output = self.activation_func(input)\n","        return self.output\n","\n","    def backward(self, grad_output):\n","        # Compute the gradient of the activation function with respect to its input\n","        return grad_output * self.activation_prime(self.input)\n"]},{"cell_type":"markdown","metadata":{"id":"26MtSuVpkomh"},"source":["**Loss**  \n","Implemented both *mean standard error* and *categorical cross entropy* activation.  \n","Forward pass computes the loss wrt *y_true* and *y_pred*.  \n","Backword pass computes the gradient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1CbW8lD8Krt"},"outputs":[],"source":["class Loss(object):\n","    def __init__(self, loss_type):\n","        # Initialize the loss function based on the loss type\n","        if loss_type == \"mse\":\n","            self.loss_func = self.mse_loss\n","            self.loss_prime = self.mse_loss_prime\n","        elif loss_type == \"cross_entropy\":\n","            self.loss_func = self.cross_entropy_loss\n","            self.loss_prime = self.cross_entropy_loss_prime\n","\n","    def mse_loss(self, y_true, y_pred):\n","        # Compute the mean squared error (MSE) loss function\n","        return np.mean((y_true - y_pred) ** 2)\n","\n","    def mse_loss_prime(self, y_true, y_pred):\n","        # Compute the derivative of the mean squared error (MSE) loss function with respect to its input\n","        return -2 * (y_true - y_pred)\n","\n","    def cross_entropy_loss(self, y_true, y_pred):\n","        # Compute the cross-entropy loss function\n","        return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n","\n","    def cross_entropy_loss_prime(self, y_true, y_pred):\n","        # Compute the derivative of the cross-entropy loss function with respect to its input\n","        return -y_true / y_pred + (1 - y_true) / (1 - y_pred)\n","\n","    def forward(self, y_true, y_pred):\n","        # Compute the forward pass through the loss function\n","        self.y_true = y_true\n","        self.y_pred = y_pred\n","        self.output = self.loss_func(y_true, y_pred)\n","        return self.output\n","\n","    def backward(self, y_true, y_pred):\n","        # Compute the gradient of the loss function with respect to its input\n","        return self.loss_prime(y_true, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"M2adfh5GlGRd"},"source":["**MLP Class**  \n","Supports arbitrary number of hidden layers. Pass a list *hidden_layers = [16, 32, 64]* to intialize the 3 hidden layers with 16, 32 and 64 neurons per layer.  \n","Forward pass computes the activation from each neuron in the layer.    \n","Backword pass computes the loss at the final layer and the gradients for weight and biases. It then backpropogates the gradient to update the weight and biases.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDT_YYMuApo3"},"outputs":[],"source":["class MLP:\n","    def __init__(self, input_dim, hidden_layers, output_dim, activation_type, loss_type):\n","        self.layers = []\n","        self.num_layers = len(hidden_layers) + 1\n","        \n","        # Initialize input layer\n","        self.layers.append(Layer(input_dim, hidden_layers[0]))\n","        \n","        # Initialize hidden layers\n","        for i in range(self.num_layers - 2):\n","            self.layers.append(Layer(hidden_layers[i], hidden_layers[i+1]))\n","        \n","        # Initialize output layer\n","        self.layers.append(Layer(hidden_layers[-1], output_dim))\n","        \n","        # Initialize activation function\n","        self.activation = Activation(activation_type)\n","        \n","        # Initialize loss function\n","        self.loss = Loss(loss_type)\n","    \n","    def forward(self, X):\n","        A = X\n","        for layer in self.layers:\n","            Z = layer.forward(A)\n","            A = self.activation.forward(Z)\n","        return A\n","    \n","    def backward(self, X, y, y_pred, learning_rate):\n","        dA_prev = self.loss.backward(y, y_pred)\n","        for layer in reversed(self.layers):\n","            dZ = self.activation.backward(layer.output)\n","            dA = np.dot(dA_prev, layer.weights.T)\n","            dW = np.dot(layer.input.T, dA_prev * dZ) / X.shape[0]\n","            db = np.mean(dA_prev * dZ, axis=0)\n","            layer.update_weights(dW, db, learning_rate)\n","            dA_prev = dA\n","    \n","    def train(self, X, y, learning_rate, epochs):\n","        for i in range(epochs):\n","            y_pred = self.forward(X)\n","            self.backward(X, y, y_pred, learning_rate)\n","    \n","    def predict(self, X):\n","        return self.forward(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UC9Ay-OhAsQ2"},"outputs":[],"source":["import numpy as np\n","# from mlp import MLP\n","\n","# Generate a random dataset of size (1000, 10)\n","X_train = np.random.rand(1000, 10)\n","y_train = np.random.rand(1000, 1)\n","\n","# Define the parameters of the MLP\n","input_dim = 10\n","hidden_layers = [10] * 10\n","output_dim = 1\n","activation_type = 'sigmoid'\n","loss_type = 'cross_entropy'\n","learning_rate = 0.001\n","epochs = 1000\n","\n","# Create an instance of the MLP class\n","mlp = MLP(input_dim, hidden_layers, output_dim, activation_type, loss_type)\n","\n","# Train the MLP on the dataset\n","mlp.train(X_train, y_train, learning_rate, epochs)\n","\n","# Make predictions on the same dataset\n","y_pred = mlp.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO85MMpqCc_l"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","\n","wine_dataset = load_diabetes()\n","X, y = wine_dataset.data, wine_dataset.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1681590699772,"user":{"displayName":"Amal Joseph","userId":"02989210271660975792"},"user_tz":-330},"id":"NIzfZvSvmmSP","outputId":"4dc90ef5-2ab8-485d-9c0f-5a893f5d62e2"},"outputs":[{"data":{"text/plain":["((353, 10), (353,))"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, y_train.shape"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":726,"status":"ok","timestamp":1681591539535,"user":{"displayName":"Amal Joseph","userId":"02989210271660975792"},"user_tz":-330},"id":"bb-04QK5aqty"},"outputs":[],"source":["input_dim = X_train.shape[1]\n","hidden_layers = [8, 16, 32, 64, 128, 64, 32, 16, 8]\n","output_dim = 1\n","activation_type = 'sigmoid'\n","loss_type = 'mse'\n","learning_rate = 0.005\n","epochs = 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B_0-qQFunMee"},"outputs":[],"source":["mlp = MLP(input_dim, hidden_layers, output_dim, activation_type, loss_type)\n","\n","# Train the MLP on the dataset\n","mlp.train(X_train, y_train.reshape(-1, output_dim), learning_rate, epochs)\n","\n","# Make predictions on the same dataset\n","y_pred = mlp.predict(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vhyLB61jnPas"},"outputs":[{"name":"stdout","output_type":"stream","text":["169.56369613520278\n"]}],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import mean_squared_error\n","\n","print(mean_squared_error(y_train.reshape(-1, output_dim), y_pred, squared=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJPVDLasoOWh"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOA5xfeVdpt+UsmaV7gFotu","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}